from sklearn.metrics import mean_absolute_error, mean_squared_error
def fill_exogeneous_features_v2(df, target_features, exog_ft, val_fh, apply_fourier = True, debug=False):
    unique_id = df['unique_id'].unique()[0]
    columns_imputed = []
    df_summary = pd.DataFrame()
#     exog_ft_fourier = exog_ft

    n_trials = 10
    n_parallel = 1
    seed = 42
#     lags_to_consider = [1, 2, 3, 4, 12]

    for column in target_features:
        if debug:
            print(f"\nProcessing column: {column}")
            print(df[[column, 'ds']].tail(20))

        series = df[column]
        first_valid_pos1 = series.first_valid_index()
        if debug:
            print('First valid index:', first_valid_pos1)

        series.iloc[:first_valid_pos1] = 0
        nan_pos = series.isna().idxmax() if series.isna().any() else None

        if nan_pos is None:
            continue  # skip columns without NaNs

        n_nan = series.iloc[nan_pos:].isna().sum()
        if debug:
            print(f"n_nan: {n_nan}")
        
        # Prepare data
        exog_ft_fourier = []
        if apply_fourier:
            uni_models = {
                'xgboost': AutoXGBoost(),
#                  'autoridge': AutoRidge(),
#                  'randomforest':AutoRandomForest(),
#         'elasticnet':AutoElasticNet(),
            }
            period = 52
            order = 4
            fourier = Fourier(period=period, order = order)
            fourier_features = fourier.in_sample(index = df['ds']).reset_index()
            df_fourier = df.merge(fourier_features, on=['ds'], how='left')
#             print(df.columns)
            exog_ft_fourier = exog_ft+['sin(1,52)','cos(1,52)','sin(2,52)','cos(2,52)','sin(3,52)','cos(3,52)','sin(4,52)','cos(4,52)']
            df_copy = df_fourier[['ds', 'unique_id', column]+exog_ft_fourier].copy()
        else:
            uni_models = {
#                 'xgboost': AutoXGBoost(),
                 'autoridge': AutoRidge(),
#                  'randomforest':AutoRandomForest(),
#         'elasticnet':AutoElasticNet(),
            }
            df_copy = df[['ds', 'unique_id', column]+exog_ft].copy()
        
            
        df_copy.rename(columns={column: 'y'}, inplace=True)
        
        print('--------------------------applied fourier----------------------------')
    

        # Split data
        train_data = df_copy.iloc[:len(df_copy)-n_nan-val_fh]
        val_data = df_copy.iloc[len(df_copy)-n_nan-val_fh:len(df_copy)-n_nan]
        test_data = df_copy.iloc[-n_nan:]
        train_holdout_data = pd.concat([train_data, val_data])
        if debug:
            print(f"Train: {train_data.shape}, Val: {val_data.shape}, Test: {test_data.shape}")
            
        print('------------------------calculating lags to consider--------------------------')
        periods, strength = get_best_lag_feats(train_data, unique_id=unique_id,column='y')
#         print(periods)
        lags_to_consider = [int(i) for i in periods]
        print('-------------------------lags and strengths calculated-----------------------')
            
        

        # Define configs
        def my_init_config(trial):
            rolling_window_size = trial.suggest_int('rolling_window_size',4,13)  # Test rolling sizes from 4 to 26
            lag_transforms = [#         ExponentiallyWeightedMean(alpha=0.3),
            RollingMean(window_size=4),
            RollingMean(window_size=rolling_window_size)  # Dynamically tune rolling mean
            ]
            return {
                'lags': lags_to_consider,
                'lag_transforms': {4: lag_transforms}
            }

        def my_fit_config(trial):
            return {'static_features': []}

        # Train AutoML on train
        fcst = AutoMLForecast(
            models=uni_models,
            freq='W-FRI',
            num_threads=n_parallel,
            init_config=my_init_config,
            fit_config=my_fit_config
        ).fit(
            train_data[['ds', 'unique_id','y']+exog_ft],
            n_windows=3,
            h=val_fh,
            num_samples=n_trials,
            fitted=True,
            study_kwargs={'sampler': optuna.samplers.TPESampler(seed=seed)}
        )
#         print(train_data)
        X_df = val_data.drop(columns=['y'])
        print(val_data.columns,'-----------------------------------------------------------------------')
#         print(X_df)
        # Forecast on validation
        if len(exog_ft)!=0:
            forecast_val_df = fcst.predict(h=val_fh,X_df=X_df)
        else:
            forecast_val_df = fcst.predict(h=val_fh)
            
        merged_val = forecast_val_df.merge(val_data[['ds', 'y']], on='ds', how='left')
        merged_val.dropna(inplace=True)
        
        model_performance = []
        y_true = merged_val['y']
        for model in uni_models.keys():
            if model in merged_val.columns:
                y_pred = merged_val[model]
                mae = mean_absolute_error(y_true, y_pred)
                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                mape = np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-8, None))) * 100  # avoid division by 0
                model_performance.append((model, mae, rmse, mape))
        
        print('-------------------------- performance of all models ---------------------------')
        print(model_performance)

        # Retrain on train+val
        models_to_retrain = {
            model_name: fcst.models_[model_name].models_[model_name]
            for model_name in fcst.models_
        }

        fcst_final = MLForecast(
            models=models_to_retrain,
            lags=lags_to_consider,
#             date_features=['month'],
            freq='W-FRI',
        ).fit(train_holdout_data[['ds', 'unique_id', 'y']+exog_ft], static_features=[], fitted=True)
        X_df_fin = test_data.drop(columns=['y'])
        if len(exog_ft)!=0:
            forecast_test_df = fcst_final.predict(h=n_nan,X_df=X_df_fin)
        else:
            forecast_test_df = fcst_final.predict(h=n_nan)
            
#         print('--------------forecast_val_df--------------------')
#         print(forecast_val_df)
#         print('------------------------forecast_test_df---------------')
#         print(forecast_test_df)

        if debug:
            fig, ax = plt.subplots(figsize=(12, 5))

            # Plot actual values
            ax.plot(df_copy['ds'], df_copy['y'], label='Actual', color='black', linewidth=2)

            # Plot validation forecasts
#             for model_name in uni_models.keys():
#                 if model_name in forecast_val_df.columns:
#                     ax.plot(forecast_val_df['ds'], forecast_val_df[model_name],
#                             linestyle='--', label=f'Val Forecast - {model_name}')

            # Plot test forecasts
            for model_name in uni_models.keys():
                if model_name in forecast_test_df.columns:
                    ax.plot(forecast_test_df['ds'], forecast_test_df[model_name],
                            linestyle='-', label=f'Test Forecast - {model_name}')

            # Mark forecast start
#             ax.axvline(x=df_copy['ds'].iloc[len(df_copy) - n_nan], color='red', linestyle='--', label='Forecast Start')

            ax.set_title(f'{column} - Forecast Comparison Across Models')
            ax.set_xlabel("Date")
            ax.legend()
            ax.grid(True)
            plt.tight_layout()
            plt.show()
            
        best_model_name = min(model_performance, key=lambda x:x[3])[0]
        print('Best model = ',best_model_name)


        replicate = forecast_test_df[best_model_name]
        df[column].iloc[nan_pos:] = replicate.values

        df_dummy = pd.DataFrame({
            'unique_id': unique_id,
            'Feature_Name': column,
            'n_imputations': n_nan,
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape
        }, index=[0])

        df_summary = pd.concat([df_summary, df_dummy], ignore_index=True)
        columns_imputed.append(column)

    print("Imputed columns:", columns_imputed)
    if debug:
        print("Summary:\n", df_summary)
#     print('----------Finally the features used are---------------------------')
#     print(exog_ft)

    return df, columns_imputed, df_summary
