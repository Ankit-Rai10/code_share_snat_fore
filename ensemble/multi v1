def get_multi_ml_forecasts_auto(train_data, holdout_data, train_holdout_data, 
                                future_data, val, fh, lags_to_consider,Features_name,
                                models = multi_models , 
                                show_plots=False, debug=False):
    code = train_data['unique_id'].values[0]
    code = code.replace('/', '_')
    def my_init_config(trial):
        rolling_window_size = trial.suggest_int('rolling_window_size',2,6)  # Test rolling sizes from 4 to 13
#             print(rolling_window_size)
        lag_transforms = [
            RollingMean(window_size=rolling_window_size)  # Dynamically tune rolling mean
        ]


        # Create a dictionary where lags greater than 6 weeks get transformed
        lag_transforms_dict = {
            lag: lag_transforms for lag in lags_to_consider if lag>7
        }

        return {
            'lags': lags_to_consider+[1],
            'lag_transforms': lag_transforms_dict
        }

    def my_fit_config(trial: optuna.Trial):
        return {
            'static_features': [],
            # 'max_horizon':fh ## switched it off because we need to take insample values for ML methods for hierarchical and this would give fh insample values for each ds
            # 'verbosity':0
        }

    fcst= AutoMLForecast(
        models= multi_models,
        freq='W-FRI',
        # season_length=13,
        num_threads=n_parallel,
#         num_threads = 1,
        init_config=my_init_config,
        fit_config=my_fit_config 
    ).fit(
        train_data,
        n_windows=3,
        h=4,
        num_samples=n_trials, ## n trials to run
        fitted=True,
        study_kwargs = {'sampler' : optuna.samplers.TPESampler(seed=seed)}, 
#         optimize_kwargs = {'sampler' : optuna.samplers.TPESampler(seed=seed)}
    )
   
    # train_data_processed = fcst.preprocess(train_val_data) ## to get the exact data format used to train model
    forecast_holdout_df = fcst.predict(h=val, X_df=holdout_data)#  level=[95] parameter for prediction interval
    train_insample_df = fcst.forecast_fitted_values()
    if debug == True:
        print("train_insample_df : ", train_insample_df.tail())
    models_to_retrain = {}
    for model1 in fcst.models_:
        models_to_retrain[model1] = fcst.models_[model1].models_[model1]
    fcst1 = MLForecast(models = models_to_retrain, lags= lags_to_consider,  # this won't be tuned
         freq='W-FRI' #,num_threads =1,
            ).fit( train_holdout_data,static_features=[], fitted=True)

    ## QC if the same model is getting trained or not
#     for model1 in fcst.models_:
#         # print(fcst.models_[model1].models_[model1])
#         # print(fcst1.models_[model1])
#         print("same params : ", fcst.models_[model1].models_[model1].get_xgb_params() == fcst1.models_[model1].get_xgb_params())
#         print('same models : ', fcst.models_[model1].models_[model1] == fcst1.models_[model1])
    if debug == True:
        print("future_data : ", future_data.shape)
    
    forecast_fh_df = fcst1.predict(h=fh, X_df=future_data) 
    forecast_fh_df['scenario']='Org'
    Features_name = [col for col in future_data.columns if col not in ['ds', 'unique_id','n_holidays','month','week','y_lag_13','Holiday Adjustment', 'Site_Access_Rate','market_share','growth_rate','trend_rate']]
    
    for col in Features_name:
        future_data_mod=future_data.copy()
        future_data_mod[col]=future_data_mod[col]*1.10
        forecast_fh_new = fcst1.predict(h=fh, X_df=future_data_mod)
        forecast_fh_new['scenario']=col+'_1.10'
        forecast_fh_df=pd.concat([forecast_fh_df,forecast_fh_new],axis=0)
    
    
    train_holdout_insample_df = fcst1.forecast_fitted_values()
    if debug == True:
        print("train_holdout_insample_df : ", train_holdout_insample_df['ds'].min(),  train_holdout_insample_df['ds'].max())
        print('pred after retrain: ', fcst1.predict(h=fh, X_df=future_data))
    fcst1.save('./saved_models/multi_ML_Auto_model_'+code)
    feat_importance_df = pd.DataFrame()
    for model1 in fcst1.models_:
        if debug == True:
            print(model1)
        X = fcst1.preprocess(train_holdout_data, static_features=[])
        # X = train_val_data.copy()

        X.drop(['ds', 'unique_id','y'], axis=1, inplace=True)
        print(X.columns)
        # 3. SHAP: Explain the feature importance using SHAP values
        final_model = fcst1.models_[model1]
        explainer = shap.Explainer(final_model, X)
        shap_values = explainer(X)
        if debug == True:
            print('X.columns : ', X.columns)
        if show_plots:
            # Plot SHAP summary plot for feature importance
            shap.summary_plot(shap_values, X,  plot_type="bar")
            plt.title('SHAP plot for ' +model1)
            plt.show()
        # 4. Compute average SHAP values (absolute values)
        average_shap_values = np.abs(shap_values.values).mean(axis=0)

        # 5. Create a DataFrame for easier interpretation
        shap_df = pd.DataFrame({
            'Feature_Name': X.columns,
            'Average_SHAP_Value': average_shap_values
        })

        # Sort features by average SHAP value (importance)
        shap_df = shap_df.sort_values(by='Average_SHAP_Value', ascending=False)
        shap_df = shap_df.reset_index()
        shap_df.drop('index', axis=1, inplace=True)

#         shap_df['cumulative_SHAP']= shap_df['Average_SHAP_Value'].cumsum()
#         shap_df['cum_imp_percentage']=shap_df['cumulative_SHAP']*100/shap_df['Average_SHAP_Value'].sum()  
#         print(shap_df)
        
        shap_df['unique_id'] = unique_id
        shap_df['model'] = model1
        feat_importance_df = pd.concat([feat_importance_df,  shap_df], ignore_index=True)
        
    return forecast_holdout_df, train_insample_df,forecast_fh_df , train_holdout_insample_df, feat_importance_df
