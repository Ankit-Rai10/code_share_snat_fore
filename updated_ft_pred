import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

def impute_future_exogenous_features(df, feature_cols_to_impute, steps_ahead=6, test_size=0.2):
    def create_time_features(df, date_col):
        df = df.copy()
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.set_index(date_col)
        df['month'] = df.index.month
        df['quarter'] = df.index.quarter
        df['week'] = df.index.isocalendar().week
        df['day_of_week'] = df.index.dayofweek
        df['day_of_month'] = df.index.day
        df['year'] = df.index.year
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)
        df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)
        df['is_month_end'] = df.index.is_month_end.astype(int)
        df['is_quarter_end'] = df.index.is_quarter_end.astype(int)
        df['is_year_end'] = df.index.is_year_end.astype(int)
        return df

    def create_lag_features(df, cols, lags=[1, 2, 3, 4, 12]):
        df = df.copy()
        for col in cols:
            if col in df.columns:
                for lag in lags:
                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)
                for win in [3, 6, 12]:
                    df[f'{col}_roll_{win}'] = df[col].rolling(win).mean()
                df[f'{col}_pct_change'] = df[col].pct_change().replace([np.inf, -np.inf], np.nan)
                df[f'{col}_diff'] = df[col].diff()
        return df

    # --- Feature Engineering ---
    df_features = create_time_features(df, df.index.name or 'ds')
    df_features = create_lag_features(df_features, feature_cols_to_impute)

    # Feature selection (exclude raw, diff, unique_id, etc.)
    feature_cols = [
        col for col in df_features.columns
        if col not in feature_cols_to_impute and not col.endswith('_diff') and col != 'unique_id'
    ]

    # Train/Test split
    split_idx = int(len(df_features) * (1 - test_size))
    train_data = df_features.iloc[:split_idx]
    test_data = df_features.iloc[split_idx:]

    xgb_models = {}
    imputation_results = {}

    for col in feature_cols_to_impute:
        print(f"Training model for {col} imputation...")

        X_train = train_data[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)
        y_train = train_data[col].fillna(0)
        X_test = test_data[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)
        y_test = test_data[col].fillna(0)

        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        model.fit(X_train, y_train)

        xgb_models[col] = model

        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        imputation_results[col] = {
            'MAE': mae,
            'RMSE': rmse,
            'Feature_Importance': dict(zip(feature_cols, model.feature_importances_))
        }

        # --- Plotting actual vs predicted ---
        plt.figure(figsize=(10, 4))
        plt.plot(y_test.values, label='Actual', marker='o')
        plt.plot(y_pred, label='Predicted', marker='x')
        plt.title(f"Actual vs Predicted for {col}")
        plt.xlabel("Time Steps")
        plt.ylabel(col)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    # --- Future Imputation ---
    future_forecasts = {}
    last_row = df_features.iloc[-1:].copy()
    last_date = df_features.index[-1]
    future_dates = pd.date_range(start=last_date + pd.offsets.MonthBegin(), periods=steps_ahead, freq='MS')

    for step, future_date in enumerate(future_dates):
        step_forecast = {}
        for col in feature_cols_to_impute:
            X_pred = last_row[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)
            pred = xgb_models[col].predict(X_pred)[0]
            step_forecast[col] = pred
            last_row[col] = pred

        for col in feature_cols_to_impute:
            for lag in [1, 2, 3, 4, 12]:
                last_row[f'{col}_lag_{lag}'] = last_row[col]
            for win in [3, 6, 12]:
                last_row[f'{col}_roll_{win}'] = last_row[col]
            last_row[f'{col}_pct_change'] = 0

        last_row.name = future_date
        future_forecasts[future_date] = step_forecast

    return future_forecasts, imputation_results
